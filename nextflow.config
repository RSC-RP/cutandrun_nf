//working directory for temporary/intermediate files produced in the workflow processes
workDir = "$HOME/temp" 

//global parameters
params {
    // general options
    sample_sheet                = "./test_data/test_dataset_sample_sheet.csv"
    queue                       = 'sceaq'
    project                     = '571397c6-5386-487d-b346-0aee560dee0b'
    outdir                      = "./results/mouse"
    peaks_outdir                = "${params.outdir}/peak_call_params3"
    publish_dir_mode            = 'copy'

    //Bowtie params
    build_index                 = false
    fasta                       = '/gpfs/shared_data/Bowtie2/mm39.fa' //required
    index                       = '/gpfs/shared_data/Bowtie2/mm39_index'
    save_unaligned              = false

    //spike-in params
    build_spike_index           = false
    spike_fasta                 = '/gpfs/shared_data/Bowtie2/GCF_000005845.2_ASM584v2_genomic.fa'
    spike_index                 = '/gpfs/shared_data/Bowtie2/ecoli_index'

    //trimgalore module specific parameters
    clip_r1                     = 0
    clip_r2                     = 0
    three_prime_clip_r1         = 0
    three_prime_clip_r2         = 0

    //Picard module specific parameters
    remove_dups                 = false

    //Effective genome size 
    gsize                       = 1.87e9 //should NOT be scientific notation to be used with deeptools

    //SEACR params
    threshold                   = 0 //any value > 0 will use threshold, even if IgG is available in sample sheet
    spike_norm                  = false
    chrom_sizes                 = '/gpfs/shared_data/Bowtie2/mm39.chrom.sizes'
    scale_factor_constant       = 10000 //scientific notation NOT allowed

    //MACS2 and khmer params
    run_macs2                   = false
    run_khmer                   = false
    kmer_size                   = 150 //kmer_size is the read-length
}

// Computational resource allocation for the processes run in the workflow
process {
    publishDir = [
        path: { "${params.outdir}/${task.process.tokenize(':')[-1].toLowerCase()}" },
        mode: params.publish_dir_mode,
        saveAs: { filename -> filename.equals('versions.yml') ? null : filename },
        failOnError: true,
    ]
    errorStrategy = "retry"
    maxRetries = 2

    //Quality trimming process specific parameters
    withName: TRIMGALORE {
        cpus = 2
        memory = 16.GB
        ext.args = ''
    }

    //Bowtie2 aligner process specific parameters
    withName: BOWTIE2_ALIGN {
        cpus = { 2 * task.attempt }
        memory = { 32.GB * task.attempt }
        ext.args = '--local --very-sensitive-local --no-unal --no-mixed --no-discordant --phred33 -I 10 -X 700'
        ext.args2 = ''      //command line arguments for `samtools sort` if params.sort_bam == true
    }

    //SEACR peak calling resources
    withName: SEACR_CALLPEAK {
        cpus = { 1 * task.attempt }
        memory = { 16.GB * task.attempt }
        ext.args = 'non relaxed'
        publishDir = [  path: { "${params.peaks_outdir}/${task.process.tokenize(':')[-1].toLowerCase()}" },
                        mode: params.publish_dir_mode,
                        failOnError: true,
                    ]
    }

    //MACS2 peak calling resources
    withName: MACS2_CALLPEAK {
        cpus = { 1 * task.attempt }
        memory = { 16.GB * task.attempt }
        ext.args = '-q 0.1 --keep-dup all --broad'
        publishDir = [  path: { "${params.peaks_outdir}/${task.process.tokenize(':')[-1].toLowerCase()}" },
                        mode: params.publish_dir_mode,
                        failOnError: true,
                    ]
    }

    //BAMCOVERAGE bigwig file  parameters 
    withName: DEEPTOOLS_BAMCOVERAGE {
        cpus = { 4 * task.attempt }
        memory = { 16.GB * task.attempt }
        ext.args = '--normalizeUsing CPM --centerReads --verbose'
    }

    //Picard mark duplicate reads
    withName: PICARD_MARKDUPLICATES {
        cpus = { 2 * task.attempt }
        memory = { 32.GB * task.attempt }
        ext.args = ''
    }

    //Picard to Remove duplicate reads
    withName: PICARD_RMDUPLICATES {
        cpus = { 2 * task.attempt }
        memory = { 32.GB * task.attempt }
        ext.args = '--REMOVE_DUPLICATES true'
    }

    //Samtools sort by read name parameters
    withName: SAMTOOLS_NSORT {
        cpus = { 1 * task.attempt }
        memory = { 32.GB * task.attempt }
        ext.args = '-n'
    }

    //convert bamtobed to bedgraph process specific resources
    withName: BAMTOBEDGRAPH {
        cpus = { 1 * task.attempt }
        memory = { 8.GB * task.attempt }
        ext.args = '' //for bedtools bamtobed
        ext.args2 = '' //for bedtools genomecov 
    }

    //Samtools sort by coordinate (for bedtools process below)
    withName: SAMTOOLS_SORT {
        cpus = { 1 * task.attempt }
        memory = { 32.GB * task.attempt }
        ext.args = ''
    }

    //Samtools sort by coordinate (for bedtools process below)
    withName: SAMTOOLS_INDEX {
        cpus = { 1 * task.attempt }
        memory = { 16.GB * task.attempt }
        ext.args = ''
    }

    //Bowtie2 aligner process specific parameters
    withName: BOWTIE2_BUILD {
        cpus = { 4 * task.attempt }
        memory = { 32.GB * task.attempt }
        ext.args = '--verbose'
    }

}

//Create profiles to easily switch between the different process executors and platforms.
params.enable_conda = false //set-to false except inside the PBS_conda profile
profiles {
    //For running on an interactive session on cybertron with singularity module loaded
    local_singularity {
        process.executor = 'local'
        singularity.enabled = true
    }
    //For executing the jobs on the HPC cluster with singularity containers
    PBS_singularity {
        process.executor = 'pbspro' 
        process.queue = "${params.queue}"
        process.clusterOptions = "-P ${params.project}"
        process.beforeScript = 'module load singularity'
        singularity.enabled = true
    }
    //For executing the jobs on the HPC cluster with conda environments. 
    PBS_conda {
        process.executor = 'pbspro' 
        process.queue = "${params.queue}"
        process.clusterOptions = "-P ${params.project}"
        params.enable_conda = true
    }
    //For running interactively on local macbook with docker installed. 
    local_docker {
        process.executor = 'local'
        docker.enabled = true
    }
}

//Configs for singularity containers on cybertron
singularity {
    autoMounts = true
    cacheDir = "$HOME/singularity"
    runOptions = '--containall --no-home'
}

//Use personal conda environments on cybertron if conda_enabled = true
conda {
    cacheDir = "$HOME/miniconda3/envs/"
}

//Configs for docker containers on local macbook with 64Gb memory
docker {
    temp = 'auto'
    runOptions = "--platform linux/amd64 --memory=32g --cpus=0.000"
}

//overwrite reports when the workflow is executed again 
report {
    overwrite = true
}