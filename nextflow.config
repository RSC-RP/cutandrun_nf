//working directory for temporary/intermediate files produced in the workflow processes
workDir = "$HOME/temp" 

//global parameters
params {
    // general options
    sample_sheet                = "./test_data/test_dataset_sample_sheet.csv"
    queue                       = 'sceaq'
    project                     = '207f23bf-acb6-4835-8bfe-142436acb58c'
    outdir                      = "./results/"
    publish_dir_mode            = 'copy'

    //Bowtie params
    build_index                 = false
    fasta                       = '/gpfs/shared_data/Bowtie2/mm39.fa'
    index                       = '/gpfs/shared_data/Bowtie2/mm39_index'
    save_unaligned              = false
    sort_bam                    = true

    //spike-in params
    spike_norm                  = false
    build_spike_index           = false
    spike_fasta                 = ''
    spike_index                 = ''

    //trimgalore module specific parameters
    clip_r1                     = 0
    clip_r2                     = 0
    three_prime_clip_r1         = 0
    three_prime_clip_r2         = 0

    //SEACR params
    genome_file                 = '/gpfs/shared_data/Bowtie2/mm39.chrom.sizes'
    threshold                   = 0

    //MACS2 and khmer effective genome size params
    //kmer_size is the read-length
    run_macs2                   = true
    run_khmer                   = false
    kmer_size                   = 150 
    macs2_gsize                 = 1.87e9
}

// Computational resource allocation for the processes run in the workflow
process {
    publishDir = [
        path: { "${params.outdir}/${task.process.tokenize(':')[-1].tokenize('_')[0].toLowerCase()}" },
        mode: params.publish_dir_mode,
        saveAs: { filename -> filename.equals('versions.yml') ? null : filename }
    ]
    errorStrategy = "retry"
    maxRetries = 2

    //Quality trimming process specific parameters
    withName: TRIMGALORE {
        cpus = 2
        memory = 16.GB
        ext.args = ''
    }

    //Bowtie2 aligner process specific parameters
    withName: BOWTIE2_ALIGN {
        cpus = { 2 * task.attempt }
        memory = { 32.GB * task.attempt }
        ext.args = '--local --very-sensitive-local --no-unal --no-mixed --no-discordant --phred33 -I 10 -X 700'
        ext.args2 = ''      //command line arguments for `samtools sort` if params.sort_bam == true
    }

    //SEACR peak calling resources
    withName: SEACR_CALLPEAK {
        cpus = { 1 * task.attempt }
        memory = { 16.GB * task.attempt }
        ext.args = 'norm stringent'
    }

    //MACS2 peak calling resources
    withName: MACS2_CALLPEAK {
        cpus = { 1 * task.attempt }
        memory = { 16.GB * task.attempt }
        ext.args = '-q 0.1 --keep-dup all'
    }

    //Bowtie2 aligner process specific parameters
    withName: BOWTIE2_BUILD {
        cpus = { 4 * task.attempt }
        memory = { 32.GB * task.attempt }
        ext.args = '--verbose'
    }

    //convert bamtobed to bedgraph process specific resources
    withName: BAMTOBEDGRAPH {
        cpus = { 1 * task.attempt }
        memory = { 8.GB * task.attempt }
        ext.args = ''
    }
}

//Create profiles to easily switch between the different process executors and platforms.
params.enable_conda = false //set-to false except inside the PBS_conda profile
profiles {
    //For running on an interactive session on cybertron with singularity module loaded
    local_singularity {
        process.executor = 'local'
        singularity.enabled = true
    }
    //For executing the jobs on the HPC cluster with singularity containers
    PBS_singularity {
        process.executor = 'pbspro' 
        process.queue = "${params.queue}"
        process.clusterOptions = "-P ${params.project}"
        process.beforeScript = 'module load singularity'
        singularity.enabled = true
    }
    //For executing the jobs on the HPC cluster with conda environments. 
    PBS_conda {
        process.executor = 'pbspro' 
        process.queue = "${params.queue}"
        process.clusterOptions = "-P ${params.project}"
        params.enable_conda = true
    }
    //For running interactively on local macbook with docker installed. 
    local_docker {
        process.executor = 'local'
        docker.enabled = true
    }
}

//Configs for singularity containers on cybertron
singularity {
    autoMounts = true
    cacheDir = "$HOME/singularity"
    runOptions = '--containall --no-home'
}

//Use personal conda environments on cybertron if conda_enabled = true
conda {
    cacheDir = "$HOME/miniconda3/envs/"
}

//Configs for docker containers on local macbook with 64Gb memory
docker {
    temp = 'auto'
    runOptions = "--platform linux/amd64 --memory=32g --cpus=0.000"
}

//overwrite reports when the workflow is executed again 
report {
    overwrite = true
}