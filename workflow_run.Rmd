---
title: "Nextflow 'rnaseq_count_nf' Pipeline Run Instructions"
author: "Jenny L Smith"
date: "`r Sys.Date()`"
always_allow_html: true
output:
  html_document:
    theme: yeti
    highlight: breezedark
    toc: true
    toc_float: true
    toc_depth: 3
    number_sections: true
    fig_caption: true
    df_print: paged
---

# Set-up 

```{r set-up, eval=TRUE, echo=TRUE}
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
```

```{r eval=TRUE}
knitr::opts_chunk$set(
  tidy.opts = list(width.cutoff = 50),
  tidy = TRUE,
  fig.align = "center",
  fig.width = 10, fig.height = 10,
  eval = FALSE
)

options(stringsAsFactors = FALSE, max.print = 100)
table <- function(..., useNA = "ifany") base::table(..., useNA = useNA)
```

```{r}
library(dplyr)
```

# Set-up  Nextflow Environment 

First, grab a compute node and activate the conda environment. It may also be best practice to use tmux or screen to ensure that if at the session is disconnected, then you're nextflow workflow (if running) won't end with SIGKILL error. 

```{bash}
#on a terminal on the Cybertron login nodes
git clone https://childrens-atlassian/bitbucket/projects/RPDEV/repos/rnaseq_count_nf/
```

```{bash}
#grab a compute note
qsub -I -q freeq -l select=1:ncpus=1:mem=8g -l walltime=8:00:00 -P [PROJECT CODE]
cd /path/to/cloned/rnaseq_count_nf
```

```{bash}
conda env create -f env/nextflow.yaml
conda activate nextflow
```

# Test the Workflow 

## Paired end

Determine if the workflow works on your installation of the conda environment by running the following command.  

```{bash}
./main_run.sh "paired_end_test"
```

## single-end test 

To test the single-end sheet, modify the sample_sheet parameter in the `nextflow.config`,

```
params {
    // general options
    sample_sheet                = "test_data/single_end_sheet.csv"
<...continues...>
}
```

then run the command

```{bash}
./main_run.sh "single_end_test"
```


## sra download

To test the sra sample sheet - modify these two lines in the `nextflow.config`.  

```
params {
    // general options
    sample_sheet                = "test_data/sra_sample_sheet.csv"
    download_sra_fqs            = true
<...continues...>
}
```

then run on the command:

```{bash}
./main_run "sra_test"
```


# Define Input Files 

## Sample Sheet

A comma delimited (csv) sample sheet is required for the input samples to be processed. Please note, ***do not remove*** the comment lines that begin with "#" in the example files.
Indeed, the 4 comment lines *must* be included in any input sample sheet. 

It must have the column names (in any order):

  * r1 - the filepath for the read 1 fastq in paired-end RNA-seq, or the single-end fastq file
    
  * r2 - the filepath for the read 2 fastq in paired-end RNA-seq
    
  * id - unique sample ID, no duplicates allowed in the sample sheet
    
  * single_end - boolean [true/false] if the data is single-end or paired-end 


**TO DO:** A function provided here, but it may not meet the needs of every experiment. 


The two examples are provided here to look at: 
```{r eval=TRUE}
example_sheet <- read.csv(here::here("test_data/paired_end_sample_sheet.csv"),
                            header = TRUE, 
                            comment.char = "#")
example_sheet
```

If downloading the fastq files directly from the SRA, the sample sheet only requires the `id` and the `single_end` columns. 

```{r eval=TRUE}
sra_example <- read.csv(here::here("test_data/sra_sample_sheet.csv"),
                        header = TRUE, 
                        comment.char = "#")

sra_example
```


## Nextflow Config

Edit the `nextflow.config` file to include the appropriate filepaths for the samples to be included in the pipeline, and the appropriate genome references. The required files are listed here:

```
workDir = "PATH/TO/SCRATCH" 

//global parameters
params {
    // general options
    sample_sheet                = "PATH/TO/SAMPLE_SHEET"
    download_sra_fqs            = [true/false]
    queue                       = 'NAME OF QUEUE'
    project                     = 'PROJECT CODE'
    outdir                      = "PATH/TO/RESULTS"

    //star specific params. Must be full filepaths for files outside the projectDir
    index                       = 'PATH/TO/STAR_INDEX/' [optional if build_index == true]
    build_index                 = false
    gtf                         = 'PATH/TO/GTF'
    fasta                       = 'PATH/TO/FASTA' [optional if build_index == false ]
    star_ignore_sjdbgtf         = false

    //trimgalore module specific parameters
    trim                        = [true/false]

    //RSEQC specific parameters
    gene_list                   = 'PATH/TO/RSEQC/rRNA.bed'
    ref_gene_model              = 'PATH/TO/RSEQC/gene_model.bed'
}
```

The `gene_list` and `ref_gene_model` can be found at the RSEQC [documentation page](http://rseqc.sourceforge.net/#download-gene-models-update-on-12-14-2021)
save these to the `/gpfs/shared_data` to share with the RSC team, if needed. It also is best practice to save the STAR indexes to `/gpfs/shared_data`. 

However, the ref_gene_model must be in BED12 format and must match the transcript IDs used in the GTF for STAR aligner. The BED12 can be produced from the GFT file using UCSC utilities (Kent tools).

**TO DO** Add a subworkflow for the creation of a BED12 compatible file for RSEQC. 


## Advanced Options

**TO DO:** add "Advanced Options" section to describe the `ext.args` under the process scope. 

# Run the workflow 

Then execute a wrapper around the `nextflow run main.nf` command which is in the `main_run.sh` shell script. Provide a descriptive name (string) for your workflow run, in this example we will use "my_analysis". 

The `main_run.sh` defines the profiles for difference executors in the variable `NFX_PROFILE`. The best choice is "PBS_singularity" which executes the jobs on the HPC using the PBS scheduler and then will run the job inside singularity containers with the appropriate software versions. 

```{bash}
./main_run.sh "my_analysis"
```

Typically, you will not need to change the `main_run.sh` often. However, if needed for testing or development, the `NFX_PROFILE` can be changed to run locally, or with docker, or with conda. You can also change the entry_point for the workflow to run only the index building step using `NFX_ENTRY` set to `star_index`. 


# Expected Outputs 

Under the path provided in the nextflow config for params "outdir", you will find directories named for each of the modules. Lets say "params.outdir = ./results". There will be the following file structure:

results/

  * fastqc/ 
    * fastqc_{sample_id}_/
    
  * multiqc/
    * {sample_sheet_basename}_multiqc_report_data/ 
    * collects fastqc, star alignment, and star quantification stats 
    
  * rseqc/
    * {sample_id}.rRNA_stats.out
    * {sample_id}.in.bam - rRNA reads
    * {sample_id}.Aligned.sortedByCoord.out.summary.txt
    * {sample_id}.Aligned.sortedByCoord.out.tin.xls
    * {sample_id}.read_distribution.txt
    
  * samtools/
    * bam index (.bai) file
  
  * sratools/
    * {SRR_RUN_ID}.fastq.gz
    * sratoolkit config file
    
  * star/
    * {Sample_ID_001}.Aligned.out.bam
    * {Sample_ID_001}.Log.final.out
    * {Sample_ID_001}.Log.out
    * {Sample_ID_001}.Log.progress.out
    * {Sample_ID_001}.ReadsPerGene.out.tab
    * {Sample_ID_001}.SJ.out.tab

In addition, there will be an HTML report with information on where the temp data is stored in the `workDir` path, and general run statistics such as resource utilized  versus requested, which helps with optimization. It will also provide information on how much walltime was used per sample, total CPU hours, etc. 

The HTML file is found in `reports` directory and will have the prefix defined on the command line when the `./main_run.sh "my_analysis"` was invoked, so in this example it would be named "my_analysis_{DATE}.html". 

There will also be a detailed nextflow log file that is useful for de-bugging which will also be named in this example, "my_analysis_{DATE}_nextflow.log".

Finally, the pipeline will produce a DAG - Directed acyclic graph which describes the workflow channels (inputs) and the modules. The DAG image will be saved under `dag/` directory with the name "my_analysis_{DATE}_dag.pdf". 

# Share the Data

```{bash}
RESULTS="PATH/TO/PIPELINE/RESULTS/"
OUTDIR="path/to/collabs/RSS"
rsync -av $RESULTS $OUTDIR 
```


# Cleaning up Cached Data

Nextflow has an utility to [clean](https://www.nextflow.io/docs/latest/cli.html#clean) up old work directories and logs that are no longer needed. This can be run after x amount of time to keep your workdir from getting too large or if you're running out of disk space. 

This requires the session ID or session name, which can found in the `.nextflow/history` file. 

```{bash}
cat .nextflow/history
nextflow clean -f high_sinoussi
```



# Session Information

```{r}
sessionInfo()
```


