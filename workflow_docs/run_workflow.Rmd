---
title: "Run Cut&Run Peak Calling"
always_allow_html: true
output:
  html_document:
    theme: yeti
    highlight: breezedark
    toc: true
    toc_float: true
    toc_depth: 3
    number_sections: true
    fig_caption: true
    df_print: paged
---

# Set-up 

```{r set-up, echo=FALSE}
knitr::opts_chunk$set(tidy.opts=list(width.cutoff=80),
                      tidy=TRUE,
                      fig.align='center',
                      fig.width = 10, fig.height = 10,
                      eval = FALSE)

options(stringsAsFactors = FALSE, max.print = 100)
table = function (..., useNA = 'ifany') base::table(..., useNA = useNA)
```

```{r message = FALSE, warning=FALSE, echo=FALSE, eval=TRUE}
library(stringr)
library(magrittr)
library(dplyr)
library(tidyr)
library(tibble)
```

# Define functions

```{r}
#Function to create symlinks to the original data files on active to the current working directory of the R project. 
mk_symlinks <- function(linked_dirname,filepaths){
  dir.create(linked_dirname, recursive = TRUE)
  lapply(filepaths,function(file){
    target <- file.path(linked_dirname, basename(file))
    if(!file.exists(target)){
      command <- glue::glue("ln -svf '{file}' '{target}'")
      system(command)
    }
  })
}
```

# Background Information 

SEACR Methodology:

  * [SEACR publication](https://epigeneticsandchromatin.biomedcentral.com/articles/10.1186/s13072-019-0287-4#Abs1)


Library Prep Protocol:

  * [Protocols.io](https://www.protocols.io/view/cut-amp-run-targeted-in-situ-genome-wide-profiling-14egnr4ql5dy/v3?step=113)

# About the Pipeline 

The pipeline runs the Bowtie2 alignment, quality trimming of reads with trimgalore, SEACR peak calling, and optionally MACS2 peak calling. It will also perform general QC statistics on the fastqs with [fastqc](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/) and the alignment. Finally, the QC reports are collected into a single file using [multiQC](https://multiqc.info/).

A DAG (directed acyclic graph) of the workflow is show below:

```{r echo=FALSE, eval=TRUE}
knitr::include_graphics("images/dag.png")
```


# Activate the Environment

Optional: use tmux on the cybertron login nodes. Name the session nextflow and then request an interactive session before activating the nextflow conda environment. The project codes can be found with `project info` command. 

Change the QUEUE and NAME variables in the code chunk below to be accurate for your Cybertron projects. 

```{bash optional, eval=FALSE}
tmux new-session -s nextflow
NAME="RSC_adhoc"
QUEUE="sceaq"
qsub -I -q $QUEUE -P $(project code $NAME) -l select=1:ncpus=1:mem=8g -l walltime=8:00:00
```

Next, for the conda environment to be solved, you will need to set channel_priority to flexible in your conda configs as well. To read more about conda environments and thier configurations, check out the [documentation](https://docs.conda.io/projects/conda/en/latest/commands/config.html#conda-config). 

```{bash, eval=FALSE}
conda config --describe channel_priority # print your current conda settings
conda config --set channel_priority flexible # set to flexible if not already done
```

Then create the conda environment that has the appropriate software, including `nextflow`, `nf-core`, and `graphviz`. The conda environment will need to be built only 1 time, afterwards, simply use `conda activate nextflow`.

```{bash required, eval=FALSE}
conda env create -f env/nextflow.yaml
conda activate nextflow
```


# Examine the Sample Sheet

A sample sheet in csv (comma separated values) format is used as input to the pipeline.  This sample sheet **must** have the following column names in any order:

  * "sample"
  * "sample_id"
  * "target_or_control"
  * "single_end"
  * "read1"
  * "read2"

```{r echo=FALSE, eval=TRUE}
sample_desc <- c("Any alphanumeric string for each biological sample in the dataset. Will have the same sample IDs for each antibody used. For example SAMPLE_1 has both H3K27me3 and IgG control CUT&RUN, and thus SAMPLE_1 has 1 row with the files for H3K27me3, and SAMPLE_1 has 2nd row with the files for IgG data.")

sample_id_desc <- glue::glue("Any alphanumeric string for each unique sample+condition. No duplicates allowed.  For example SAMPLE_1 has both H3K27me3 and IgG control CUT&RUN. Thus, SAMPLE_1 is the value in `sample`, and   \"SAMPLE_1_H3K27me3\" is the value in `sample_id`. Again, SAMPLE_1 has 2nd row with the files for IgG data, where SAMPLE_1 is the value in `sample`, and   \"SAMPLE_1_IgG\" is the value in `sample_id`")

target_desc <- c("Must contain the values [target or control] case-sensitive. Target is for the antibodies using the immunoprecipitation for the proteins of interest, such as transcription factors or histone modifications like H3K27me3, or the value control for the isotype control (eg IgG).")

read1 <- c("Contain absolute filepaths to  read 1 in paired-end fastqs.")
read2 <- c("Contain absolute filepaths to  read 2 in paired-end fastqs.")
single_end <- c("For CUT&RUN data it should always be [false] case-sensitive.")
```

```{r eval=TRUE, echo=FALSE}
data.frame(column_name=c("sample","sample_id", "target_or_control","read1", "read2","single_end"),
       column_description=c(sample_desc,sample_id_desc,target_desc,read1, read2, single_end)) %>% 
  DT::datatable()
```
 
Below is an example of a complete sample sheet for use in the pipeline, which can be edited for your own samples in `test_data/test_dataset_sample_sheet.csv`.
 
```{r eval=TRUE}
sample_sheet <- read.csv(here::here("test_data/test_dataset_sample_sheet.csv")) %>% 
  mutate(single_end="false") %>% 
  mutate(sample=str_split_fixed(sample_id, pattern = "_", n=3)[,1]) %>%
  select(sample, everything())

head(sample_sheet)
# dim(sample_sheet)
```


# Run the Example Data

To ensure that the pipeline works, first run the test data set. This example will run using the data found in the `test_sample_sheet.csv`. 

```{bash}
./main_run.sh "test_dataset"
```

# Configure Pipeline for Your Data

## Configuration file 

Open the configuration file `nextflow.config` and edit the necessary parameters for building the index, and/or running the alignment or peak calling steps. 

```{r eval=TRUE}
usethis::edit_file(here::here("nextflow.config"))
```

Be sure to change the following lines for the queue, project code, alignment reference files, and whether to run MACS2 calls along with the SEACR peak calling algorithm. 

```
params {
    // general options
    sample_sheet                = "./test_data/test_dataset_sample_sheet.csv"
    queue                       = 'sceaq'
    project                     = '207f23bf-acb6-4835-8bfe-142436acb58c'
    outdir                      = "./results/mouse"
    peaks_outdir                = "${params.outdir}/peak_calls"
    publish_dir_mode            = 'copy'

    //Bowtie params
    build_index                 = false
    fasta                       = '/PATH/TO/genome.fasta' [REQUIRED if build_index=true]
    index                       = '/PATH/TO/mm39_index' [REQUIRED if build_index=false]
    save_unaligned              = false
    sort_bam                    = true
    
    //spike-in params
    build_spike_index           = false
    spike_fasta                 = '/PATH/TO/ecoli_genome.fasta'
    spike_index                 = '/PATH/TO/ecoli_index'

    //trimgalore module specific parameters
    clip_r1                     = 0
    clip_r2                     = 0
    three_prime_clip_r1         = 0
    three_prime_clip_r2         = 0
    
    //Picard module specific parameters
    remove_dups                 = false

    //SEACR params
    threshold                   = 0 
    spike_norm                  = [true or false] //use spike-in normalization factor or not
    chrom_sizes                 = '/gpfs/shared_data/Bowtie2/mm39.chrom.sizes'
    scale_factor_constant       = 10000
    
    //MACS2 and khmer effective genome size params
    //kmer_size is the read-length
    run_macs2                   = [true or false]
    run_khmer                   = [true or false]
    kmer_size                   = [INTEGER of fastq read length] 
    gsize                       = [INTEGER of effective genome size, eg 3.0E+9 for mouse. Scientific notation OK.]
}
```

### Advanced Options

In the `nextflow.config`, you can define additional command line arguments to the scientific software under [process scope](https://www.nextflow.io/docs/latest/process.html#). 

Be aware the default command line parameters for `Bowtie2` processes are already provided, but can be edited. This mostly would be relevant for `SEACR` and `MACS2` parameters that often need to be re-run multiple times when deciding on the appropriate peak-set to use.

Often, you will need to change `SEACR` from "non" to "norm" for different normalization strategies, and the `MACS2` broad and narrow peak calling parameters for different histone modifications. 

Another reason to use these advanced options is to change computational resources requested. The CPUs and memory parameters can updated to request a larger amount of resources like CPUs or memory if files are large. 


In the example below, you can provide additional command line arguments to `FastQC` by adding the argument `--quiet` directly to `ext.args` as shown below, which will suppress any messages stdout and only report errors

```
process {
    <...>
    
    //FASTQC process specific parameters
    withName: FASTQC {
        cpus = 1
        memory = 16.GB
        ext.args = '--quiet'
    }
    
    <...>
    
    //SEACR peak calling resources
    withName: SEACR_CALLPEAK {
        cpus = { 1 * task.attempt }
        memory = { 16.GB * task.attempt }
        ext.args = 'non stringent'
        publishDir = [...]
    }

    //MACS2 peak calling resources
    withName: MACS2_CALLPEAK {
        cpus = { 1 * task.attempt }
        memory = { 16.GB * task.attempt }
        ext.args = '-q 0.1 --keep-dup all'
        publishDir = [...]
    }
```


# Run Script 

```{r eval=TRUE}
usethis::edit_file(here::here("main_run.sh"))
```

Decide on the `NFX_PROFILE`, which allows you to run the processes either locally, or using the PBS job scheduler on Cybertron, and determine if you'd like to use singularity containers or docker containers.

  2) `PBS_singularity` [DEFAULT, recommended]
    * you can submit a PBS job that will use singularity containers on Cybertron
    * This takes care of requesting the appropriate resources using PBS
    
  1) `local_singularity`
    * locally on an interactive session Cybertron with singularity 
    * requires appropriate computational resources be requested using `qsub -I -q <queue_name> -P <project_code> -l select=1:ncpus=4:mem=32GB`

Edit the script `main_run.sh` and change the values for the `NFX_PROFILE` variable if desired. 

```
#Options: 'local_singularity', 'PBS_singularity'
NFX_PROFILE='PBS_singularity'
```

## Alignment and Peak Calls

Edit the variables in the `main_run.sh` script for entry-point of the workflow. The default option *"align_call_peaks"* for the `NFX_ENTRY` variable will run the entire pipeline including building the index, QC, alignment, and peak calling (SEACR and/or MACS2)  if the parameter `build_index=true`. Otherwise, this entry-point will run QC, alignment, and peak calling (SEACR and/or MACS2).

If you already have aligned BAM files, see `test_data/test_dataset_bams_sample_sheet.csv` for an example to call peaks only using the entry `call_peaks`. 

```
#Options: 'bowtie2_index', 'align_call_peaks', 'call_peaks'
NFX_ENTRY='align_call_peaks'
```

Then, execute the `main_run.sh` script in order to complete the peak calling on the samples. Provide a small descriptive prefix for the pipeline run. 

```{bash}
./main_run.sh "my_analysis"
```


## Optional: Build the Index and Exit Pipeline

You can also change the entry-point of the workflow, which is accomplished by setting the `NFX_ENTRY` variable in the `main_run.sh` script  to be `bowtie2_index`. This will allow the pipeline to run only the Bowtie2 build process and exit upon completion of the index.

```
#Options: 'local_singularity', 'PBS_singularity'
NFX_PROFILE='PBS_singularity'

#Options: 'bowtie2_index' or 'call_peaks'
NFX_ENTRY='bowtie2_index'
```

```{bash}
./main_run.sh "bowtie2_index"
```


# Expected Outputs 

Under the path provided in the nextflow config for params "outdir", you will find directories named for each of the modules. Lets say `params.outdir = ./results/mouse`, and `peaks_outdir = "${params.outdir}/peak_calls"`. 

There will be the following file structure:

* results/mouse/
  
    * bamtobedgraph/

    * bowtie2_align/
        * {sample_id}.bam
        * {sample_id}.bowtie2.log
        
    * deeptools_bamcoverage/
    
    * fastqc/
    
    * fastqc_trim/
    
    * multiqc/
    
    * peak_calls/
     
        * seacr/
            * {sample_id}.[norm or non].[stringent or relaxed].bed
        
        * macs2/
            * {sample_id}_peaks.[narrowPeak or broadPeak]
            * {sample_id}_peaks.xls 
            * {sample_id}_summits.bed

    * picard_markduplicates/
    
    * samtools_index/
    
    * samtools_nsort/
    
    * samtools_sort/
    
    * spikein_align/
        * {sample_id}_spikein.bam
        * {sample_id}_spikein.bowtie2.log
        
    * trimgalore/
        * {sample_id}_1.gz_trimming_report.txt
        * {sample_id}_2.gz_trimming_report.txt
        * {sample_id}.M1_H3K27_NK_1_val_1.fq.gz
        * {sample_id}.M1_H3K27_NK_1_val_2.fq.gz

## Pipeline Reports 

In addition, there will be an HTML report with information on where the temp data is stored in the `workDir` path, and general run statistics such as resource utilized  versus requested, which helps with optimization. It will also provide information on how much walltime was used per sample, total CPU hours, etc. 

The HTML file is found in `reports` directory and will have the prefix defined on the command line when the `./main_run.sh "my_analysis"` was invoked, so in this example it would be named "my_analysis_{DATE}.html". 

There will also be a detailed nextflow log file that is useful for de-bugging which will also be named in this example, "my_analysis_{DATE}_nextflow.log".

Finally, the pipeline will produce a DAG - Directed acyclic graph,  which describes the workflow channels (inputs) and the modules. The DAG image will be saved under `dag/` directory with the name "my_analysis_{DATE}_dag.pdf". 

